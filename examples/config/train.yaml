defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

env_vars_file_path: .env

log:
  experiment_log_dir: ./logs
  metrics_log_dir: ./logs

wandb:
  enabled: true
  project: test_project
  group: test_group
  tags: ["test"]
  save_code: true

deterministic:
  random_seed: 1234
  torch_use_deterministic_algorithms: true
  cudnn_backend_deterministic: true

train_epochs: 3
step_by_epoch: false
validate_every: 100

dataloader:
  batch_size: 128
  num_workers: 4

learning_rate: 1e-04

optimizer:
  weight_decay: 0.001

lr_scheduler:
  step_size: 2500
  gamma: 0.01

# saves model state dicts throughout training phase at defined steps
# is step_by_epoch = True, and checkpoint_every = 2, then model state dict will be saved every 2 epochs
# is step_by_epoch = False, and checkpoint_every = 2, then model state dict will be saved every 2 train steps
model_checkpoint:
  checkpoint_every: 300
  save_dir: ./checkpoints

# saves latest model state dict at current train step, mainly for resume of interrupted training
# snapshot will be deleted when training is finished
model_snapshot:
  save_path: ./snapshots/snapshot.t7

# if enabled, training will resume from the model state dict in the path provided
# else, training will start from scratch
resume_training:
  enabled: false
  model_state_dict_path: ./checkpoints/checkpoint.t7
